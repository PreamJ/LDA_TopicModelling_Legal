{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import random\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import MmCorpus\n",
    "import mymodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/DatasetLegal.csv')\n",
    "str_answer = data['answer'].astype(str)\n",
    "str_answer = str_answer.map(lambda x: re.sub('[,.!?#/]', '', x))\n",
    "str_question = data['question'].astype(str)\n",
    "str_question = str_question.map(lambda x: re.sub('[,.!?*/]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense_token = []\n",
    "for i in range(len(str_answer)):\n",
    "  sentense_token.append(str_question[i])\n",
    "  sentense_token.append(str_answer[i])\n",
    "\n",
    "train_data = sentense_token[:13062]\n",
    "test_data = sentense_token[13061:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token_answer = []\n",
    "for sentense in train_data:\n",
    "  word = word_tokenize(sentense, engine='newmm')\n",
    "  word_token_answer.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['มีเรื่อง', 'ปรึกษา', 'ครอบครัว', 'สินสมรส', 'สินส่วนตัว', 'การทราบ', 'ข้อเท็จจริง', 'วิธีการ']\n"
     ]
    }
   ],
   "source": [
    "stopwords = list(thai_stopwords())\n",
    "read_stopwords = pd.read_csv('dataset/add_stopwords.csv')\n",
    "add_stopwords = read_stopwords['stopword'].values.tolist()\n",
    "processed_answer = []\n",
    "for sentense in word_token_answer:\n",
    "  each_sentense = []\n",
    "  for word in sentense:\n",
    "    if(word not in stopwords + add_stopwords):\n",
    "      each_sentense.append(word)\n",
    "  processed_answer.append(each_sentense)\n",
    "print(processed_answer[0][:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(processed_answer)\n",
    "# print(id2word)\n",
    "with open('model/id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in processed_answer:\n",
    "  vec = id2word.doc2bow(text)\n",
    "  corpus.append(vec)\n",
    "MmCorpus.serialize('model/corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 6\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       iterations=100,\n",
    "                                       chunksize=800,\n",
    "                                       passes=5,\n",
    "                                       alpha=0.9,\n",
    "                                       eta=0.5\n",
    "                                       )\n",
    "with open('model/lda_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lda_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"เงิน\" + 0.018*\"รถ\" + 0.015*\"จ่าย\" + 0.015*\"บาท\" + 0.012*\"สัญญา\" + 0.012*\"บริษัท\" + 0.010*\"แจ้ง\" + 0.010*\"เพื่อน\" + 0.010*\"ซื้อ\" + 0.009*\"ขาย\"'),\n",
       " (1,\n",
       "  '0.032*\"ศาล\" + 0.016*\"ฟ้อง\" + 0.015*\"เจ้าหนี้\" + 0.014*\"ชำระหนี้\" + 0.013*\"ตำรวจ\" + 0.011*\"ลูกหนี้\" + 0.010*\"หนี้\" + 0.010*\"จำคุก\" + 0.009*\"บาท\" + 0.008*\"ทนาย\"'),\n",
       " (2,\n",
       "  '0.026*\"สามี\" + 0.024*\"แม่\" + 0.024*\"บ้าน\" + 0.021*\"ลูก\" + 0.020*\"แฟน\" + 0.020*\"พ่อ\" + 0.016*\"ภรรยา\" + 0.011*\"บุตร\" + 0.010*\"เด็ก\" + 0.009*\"หย่า\"'),\n",
       " (3,\n",
       "  '0.057*\"ที่ดิน\" + 0.019*\"โอน\" + 0.017*\"แบ่ง\" + 0.017*\"บ้าน\" + 0.016*\"สิทธิ\" + 0.014*\"ทรัพย์สิน\" + 0.013*\"มรดก\" + 0.012*\"ผู้จัดการมรดก\" + 0.011*\"ทายาท\" + 0.011*\"ขาย\"'),\n",
       " (4,\n",
       "  '0.035*\"นายจ้าง\" + 0.030*\"บริษัท\" + 0.030*\"ลูกจ้าง\" + 0.024*\"ทำงาน\" + 0.017*\"จ่าย\" + 0.014*\"งาน\" + 0.013*\"สิทธิ\" + 0.013*\"พนักงาน\" + 0.012*\"เลิกจ้าง\" + 0.011*\"กรณี\"'),\n",
       " (5,\n",
       "  '0.047*\"มาตรา\" + 0.018*\"สิทธิ\" + 0.017*\"โจทก์\" + 0.016*\"ปพพ\" + 0.016*\"ศาล\" + 0.016*\"บุตร\" + 0.015*\"สามี\" + 0.010*\"ภริยา\" + 0.010*\"ตามกฎหมาย\" + 0.010*\"จำเลย\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(num_topics=6, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {\n",
    "    0 : \"Contract\",\n",
    "    1 : \"Labor\",\n",
    "    2 : \"Undefine :)\",\n",
    "    3 : \"Personal Right\",\n",
    "    4 : \"Family\",\n",
    "    5 : \"Succession\",\n",
    "}\n",
    "\n",
    "with open('model/topic_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(topic_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "LDAvis_data_filepath\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sections below are in the process of being updated with new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = random.choice(test_data)\n",
    "print(new_doc)\n",
    "test_doc = mymodule.preprocess(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel.load('model/lda_model.pkl')\n",
    "new_doc_topics = lda_model.get_document_topics(test_doc)\n",
    "new_doc_topics_dict = {topic_dict[topic]: prob for topic, prob in new_doc_topics}\n",
    "print(new_doc_topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gensim.corpora.MmCorpus('model/corpus.mm')\n",
    "data = pd.read_csv('dataset/DatasetLegal.csv')\n",
    "with open('model/id2word.pkl', 'rb') as f:\n",
    "  id2word = pickle.load(f)\n",
    "corpus_lda = lda_model[corpus]\n",
    "index = similarities.MatrixSimilarity(corpus_lda, num_features=len(id2word))\n",
    "sims = index[new_doc_topics]\n",
    "print(type(sims))\n",
    "sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(f\"Topic distribution for new document : {new_doc_topics}\\n{new_doc}\\n\")\n",
    "for doc_id, similarity in sims_sorted[:5]:\n",
    "    print(f\"Document ID: {doc_id}, Similarity score: {similarity}\")\n",
    "    print(data.answer[doc_id])\n",
    "    print(\"Topic distribution for similar document : \")\n",
    "    for num, dis in corpus_lda[doc_id]:\n",
    "      print(f\"\\t({topic_dict.get(num)}, {'%.5f' %dis})\")\n",
    "    #print(\"\\n\")\n",
    "    #print(f\"Topic distribution for similar document : \\n{corpus_lda[doc_id]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
