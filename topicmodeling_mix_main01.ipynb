{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import random\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import MmCorpus\n",
    "import mymodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/DatasetLegal.csv')\n",
    "str_answer = data['answer'].astype(str)\n",
    "str_answer = str_answer.map(lambda x: re.sub('[,.!?#/]', '', x))\n",
    "str_question = data['question'].astype(str)\n",
    "str_question = str_question.map(lambda x: re.sub('[,.!?*/]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense_token = []\n",
    "for i in range(len(str_answer)):\n",
    "  sentense_token.append(str_question[i])\n",
    "  sentense_token.append(str_answer[i])\n",
    "\n",
    "train_data = sentense_token[:13062]\n",
    "test_data = sentense_token[13062:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token_answer = []\n",
    "for sentense in train_data:\n",
    "  word = word_tokenize(sentense, engine='newmm')\n",
    "  word_token_answer.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(thai_stopwords())\n",
    "read_stopwords = pd.read_csv('dataset/add_stopwords.csv')\n",
    "add_stopwords = read_stopwords['stopword'].values.tolist()\n",
    "processed_answer = []\n",
    "for sentense in word_token_answer:\n",
    "  each_sentense = []\n",
    "  for word in sentense:\n",
    "    if(word not in stopwords + add_stopwords):\n",
    "      each_sentense.append(word)\n",
    "  processed_answer.append(each_sentense)\n",
    "print(processed_answer[0][:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(processed_answer)\n",
    "# print(id2word)\n",
    "with open('model/id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in processed_answer:\n",
    "  vec = id2word.doc2bow(text)\n",
    "  corpus.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 6\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       iterations=100,\n",
    "                                       chunksize=2000,\n",
    "                                       passes=10,\n",
    "                                       alpha=0.9,\n",
    "                                       eta=0.5\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/lda_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lda_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics(num_topics=6, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "topic_dict = {\n",
    "    0 : \"succession\",\n",
    "    1 : \"violation\",\n",
    "    2 : \"family\",\n",
    "    3 : \"criminal\",\n",
    "    4 : \"contract\",\n",
    "    5 : \"labor\",\n",
    "}\n",
    "\n",
    "with open('model/topic_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(topic_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim_models as gensimvis\n",
    "# num_topics = 6\n",
    "# with open('model/id2word.pkl', 'rb') as f:\n",
    "#   id2word = pickle.load(f)\n",
    "# with open('model/lda_model.pkl', 'rb') as f:\n",
    "#   lda_model = pickle.load(f)\n",
    "# pyLDAvis.enable_notebook()\n",
    "# LDAvis_data_filepath = os.path.join('./ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "# LDAvis_data_filepath\n",
    "# if 1 == 1:\n",
    "#     LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "#     with open(LDAvis_data_filepath, 'wb') as f:\n",
    "#         pickle.dump(LDAvis_prepared, f)\n",
    "# with open(LDAvis_data_filepath, 'rb') as f:\n",
    "#     LDAvis_prepared = pickle.load(f)\n",
    "# pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "# LDAvis_prepared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sections below working on lda model in question part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense_token_question = []\n",
    "# for each_question in str_question:\n",
    "#   sentense_token_question.append(each_question)\n",
    "for i in range(6531):\n",
    "  sentense_token_question.append(str_question[i])\n",
    "# print(len(question_data))\n",
    "topic_question = []\n",
    "corpus_question = []\n",
    "for sentense in sentense_token_question:\n",
    "  processed_question = mymodule.preprocess(sentense)\n",
    "  corpus_question.append(processed_question)\n",
    "  topic = lda_model.get_document_topics(processed_question)\n",
    "  topic_question.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/question_lda.pkl', 'wb') as f:\n",
    "    pickle.dump(topic_question, f)\n",
    "MmCorpus.serialize('model/corpus_question.mm', corpus_question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sections below are in the process of being updated with new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = random.choice(test_data)\n",
    "print(new_doc)\n",
    "test_doc = mymodule.preprocess(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/question_lda.pkl', 'rb') as f:\n",
    "    topic_question = pickle.load(f)\n",
    "with open('model/topic_dict.pkl', 'rb') as f:\n",
    "    topic_dict = pickle.load(f)\n",
    "new_doc_topics = lda_model.get_document_topics(test_doc)\n",
    "new_doc_topics_dict = {topic_dict[topic]: prob for topic, prob in new_doc_topics}\n",
    "print(new_doc_topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(index)\n",
    "# print(corpus_lda)\n",
    "# print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_question\n",
    "data = pd.read_csv('dataset/DatasetLegal.csv')\n",
    "with open('model/id2word.pkl', 'rb') as f:\n",
    "  id2word = pickle.load(f)\n",
    "corpus_lda = lda_model[corpus]\n",
    "index = similarities.MatrixSimilarity(corpus_lda, num_features=len(id2word))\n",
    "sims = index[new_doc_topics]\n",
    "sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(f\"Topic distribution for new document : {new_doc_topics}\\n{new_doc}\\n\")\n",
    "for doc_id, similarity in sims_sorted[:5]:\n",
    "  print(f\"Document ID: {doc_id}, Similarity score: {similarity}\")\n",
    "  print(data.answer[doc_id])\n",
    "  print(\"Topic distribution for similar document : \")\n",
    "  for num, dis in corpus_lda[doc_id]:\n",
    "    print(f\"\\t({topic_dict.get(num)}, {'%.5f' %dis})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
