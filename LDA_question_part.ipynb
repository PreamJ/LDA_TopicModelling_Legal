{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import random\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import MmCorpus\n",
    "import csv\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import mymodule\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/DatasetLegal.csv')\n",
    "str_question = data['question'].astype(str)\n",
    "str_question = str_question.map(lambda x: re.sub('[,.!?*#/]', '', x))\n",
    "\n",
    "with open('model/id2word.pkl', 'rb') as f:\n",
    "    id2word = pickle.load(f)\n",
    "with open('model/lda_model.pkl', 'rb') as f:\n",
    "    lda_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense_token_question = []\n",
    "for i in range (5625):\n",
    "  sentense_token_question.append(str_question[i])\n",
    "topic_question = []\n",
    "corpus_question = []\n",
    "for sentense in sentense_token_question:\n",
    "  processed_question = mymodule.preprocess(sentense)\n",
    "  corpus_question.append(id2word.doc2bow(processed_question))\n",
    "  topic = lda_model.get_document_topics(processed_question)\n",
    "  topic_question.append(topic)\n",
    "with open('model/question_lda.pkl', 'wb') as f:\n",
    "  pickle.dump(topic_question, f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 11:05:18.670 INFO    gensim.corpora.mmcorpus: storing corpus in Matrix Market format to corpus_question.mm\n",
      "2023-05-26 11:05:18.673 INFO    gensim.matutils: saving sparse matrix to corpus_question.mm\n",
      "2023-05-26 11:05:18.675 INFO    gensim.matutils: PROGRESS: saving document #0\n",
      "2023-05-26 11:05:18.921 INFO    gensim.matutils: PROGRESS: saving document #1000\n",
      "2023-05-26 11:05:19.122 INFO    gensim.matutils: PROGRESS: saving document #2000\n",
      "2023-05-26 11:05:19.274 INFO    gensim.matutils: PROGRESS: saving document #3000\n",
      "2023-05-26 11:05:19.464 INFO    gensim.matutils: PROGRESS: saving document #4000\n",
      "2023-05-26 11:05:19.646 INFO    gensim.matutils: PROGRESS: saving document #5000\n",
      "2023-05-26 11:05:19.772 INFO    gensim.matutils: saved 5625x15417 matrix, density=0.227% (197284/86720625)\n",
      "2023-05-26 11:05:19.776 INFO    gensim.corpora.indexedcorpus: saving MmCorpus index to corpus_question.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpora.MmCorpus.serialize('corpus_question.mm', corpus_question, id2word=id2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
