{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import random\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import MmCorpus\n",
    "import csv\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from sklearn import metrics\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/DatasetLegal.csv')\n",
    "# train_data = pd.read_csv('model/train_data.csv')\n",
    "# test_data = pd.read_csv('model/test_data.csv')\n",
    "with open(\"model/train_data.csv\", 'r') as f:\n",
    "  train_data_csv = csv.reader(f)\n",
    "  train_data = [i for i in train_data_csv]\n",
    "with open(\"model/test_data.csv\", 'r') as f:\n",
    "  test_data_csv = csv.reader(f)\n",
    "  test_data = [i for i in test_data_csv]\n",
    "with open('model/id2word.pkl', 'rb') as f:\n",
    "    id2word = pickle.load(f)\n",
    "corpus = []\n",
    "for text in train_data:\n",
    "  vec = id2word.doc2bow(text)\n",
    "  corpus.append(vec)\n",
    "pd.Series(corpus)\n",
    "with open(\"model/topic_dict.pkl\", \"rb\") as f:\n",
    "   topic_dict = pickle.load(f)\n",
    "with open(\"model/lda_model.pkl\", \"rb\") as f:\n",
    "   lda_model = pickle.load(f)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='model_callbacks.log',\n",
    "                    format=\"%(asctime)s:%(levelname)s:%(message)s\",\n",
    "                    level=logging.NOTSET)\n",
    "\n",
    "from gensim.models.callbacks import PerplexityMetric, CoherenceMetric\n",
    "perplexity_logger = PerplexityMetric(corpus=corpus, logger='shell')\n",
    "coherence_cv_logger = CoherenceMetric(corpus=corpus, logger='shell', coherence = 'c_v', texts = train_data)\n",
    "\n",
    "with open(\"model/corpus_question.pkl\", \"rb\") as f:\n",
    "   corpus_question = pickle.load(f)\n",
    "with open(\"model/corpus_answer.pkl\", \"rb\") as f:\n",
    "   corpus_answer = pickle.load(f)\n",
    "\n",
    "corpus_question_train = corpus_question[:5625]\n",
    "corpus_question_test = corpus_question[5625:]\n",
    "corpus_answer_train = corpus_answer[:5625]\n",
    "corpus_answer_test = corpus_answer[5625:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic labor : 0.74\n",
      "accuracy for topic personal rights/court : 0.92\n",
      "accuracy for topic family/succession : 0.94\n",
      "accuracy for topic contract : 0.62\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_4.pkl\", \"rb\") as f:\n",
    "    lda_model_4 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_4.pkl\", \"rb\") as f:\n",
    "    topic_dict_4 = pickle.load(f)\n",
    "num_topics = len(topic_dict_4)\n",
    "pred_question_score = [lda_model_4[text] for text in corpus_question_train]\n",
    "pred_answer_score = [lda_model_4[text] for text in corpus_answer_train]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_4[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_4[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_4[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_4[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic labor : 0.72\n",
      "accuracy for topic personal rights/court : 0.93\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.57\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_4.pkl\", \"rb\") as f:\n",
    "    lda_model_4 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_4.pkl\", \"rb\") as f:\n",
    "    topic_dict_4 = pickle.load(f)\n",
    "num_topics = len(topic_dict_4)\n",
    "pred_question_score = [lda_model_4[text] for text in corpus_question_test]\n",
    "pred_answer_score = [lda_model_4[text] for text in corpus_answer_test]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_4[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_4[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_4[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_4[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4701 docs\n",
      "count_false = 924 docs\n",
      "accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "count_true = 1160 docs\n",
      "count_false = 246 docs\n",
      "accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print('test')\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4701 docs\n",
      "count_false = 924 docs\n",
      "accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (bleu) = 0.70\n",
      "accuracy (rouge) = 0.77\n",
      "accuracy (jaccard) = 0.81\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "accuracy (bleu) = 0.68\n",
      "accuracy (rouge) = 0.75\n",
      "accuracy (jaccard) = 0.79\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "print('test')\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4194,  223],\n",
       "        [ 229,  979]],\n",
       "\n",
       "       [[2598,  212],\n",
       "        [1240, 1575]],\n",
       "\n",
       "       [[3533,  136],\n",
       "        [ 207, 1749]],\n",
       "\n",
       "       [[2803, 2006],\n",
       "        [ 131,  685]]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1090,   51],\n",
       "        [  48,  217]],\n",
       "\n",
       "       [[ 597,   42],\n",
       "        [ 357,  410]],\n",
       "\n",
       "       [[ 872,   32],\n",
       "        [  60,  442]],\n",
       "\n",
       "       [[ 644,  570],\n",
       "        [  28,  164]]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('test')\n",
    "metrics.multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                labor       0.81      0.81      0.81      1208\n",
      "personal rights/court       0.88      0.56      0.68      2815\n",
      "    family/succession       0.93      0.89      0.91      1956\n",
      "             contract       0.25      0.84      0.39       816\n",
      "\n",
      "            micro avg       0.66      0.73      0.69      6795\n",
      "            macro avg       0.72      0.78      0.70      6795\n",
      "         weighted avg       0.81      0.73      0.74      6795\n",
      "          samples avg       0.70      0.77      0.71      6795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_names = list(topic_dict_4.values())\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                labor       0.81      0.82      0.81       265\n",
      "personal rights/court       0.91      0.53      0.67       767\n",
      "    family/succession       0.93      0.88      0.91       502\n",
      "             contract       0.22      0.85      0.35       192\n",
      "\n",
      "            micro avg       0.64      0.71      0.67      1726\n",
      "            macro avg       0.72      0.77      0.69      1726\n",
      "         weighted avg       0.82      0.71      0.73      1726\n",
      "          samples avg       0.68      0.75      0.68      1726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_names = list(topic_dict_4.values())\n",
    "print('test')\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic labor : 0.69\n",
      "accuracy for topic personal_right/court : 0.91\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.73\n",
      "accuracy for topic criminal : 0.75\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_5.pkl\", \"rb\") as f:\n",
    "    lda_model_5 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_5.pkl\", \"rb\") as f:\n",
    "    topic_dict_5 = pickle.load(f)\n",
    "num_topics = len(topic_dict_5)\n",
    "pred_question_score = [lda_model_5[text] for text in corpus_question_train]\n",
    "pred_answer_score = [lda_model_5[text] for text in corpus_answer_train]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_5[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labor</th>\n",
       "      <th>perso...</th>\n",
       "      <th>family...</th>\n",
       "      <th>contract</th>\n",
       "      <th>criminal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5625 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labor  perso...  family...  contract  criminal\n",
       "0         0         0          1         0         0\n",
       "1         0         0          1         0         0\n",
       "2         0         1          0         0         0\n",
       "3         0         0          1         0         0\n",
       "4         1         0          0         1         0\n",
       "...     ...       ...        ...       ...       ...\n",
       "5620      1         1          0         1         0\n",
       "5621      0         1          0         1         0\n",
       "5622      0         0          0         1         1\n",
       "5623      0         0          1         0         1\n",
       "5624      1         0          0         0         1\n",
       "\n",
       "[5625 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(question_predict)\n",
    "df.columns = [\"labor\", \"perso...\", \"family...\", \"contract\", \"criminal\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labor</th>\n",
       "      <th>perso...</th>\n",
       "      <th>family...</th>\n",
       "      <th>contract</th>\n",
       "      <th>criminal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5623</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5625 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labor  perso...  family...  contract  criminal\n",
       "0         0         0          1         0         0\n",
       "1         0         0          1         0         0\n",
       "2         0         1          0         0         0\n",
       "3         0         0          1         0         0\n",
       "4         0         0          0         1         0\n",
       "...     ...       ...        ...       ...       ...\n",
       "5620      0         1          0         0         0\n",
       "5621      0         1          0         0         1\n",
       "5622      0         1          0         0         1\n",
       "5623      0         1          1         0         0\n",
       "5624      1         0          0         0         0\n",
       "\n",
       "[5625 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame(answer_predict)\n",
    "df.columns = [\"labor\", \"perso...\", \"family...\", \"contract\", \"criminal\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "accuracy for topic labor : 0.67\n",
      "accuracy for topic personal_right/court : 0.92\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.72\n",
      "accuracy for topic criminal : 0.75\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_5.pkl\", \"rb\") as f:\n",
    "    lda_model_5 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_5.pkl\", \"rb\") as f:\n",
    "    topic_dict_5 = pickle.load(f)\n",
    "num_topics = len(topic_dict_5)\n",
    "pred_question_score = [lda_model_5[text] for text in corpus_question_test]\n",
    "pred_answer_score = [lda_model_5[text] for text in corpus_answer_test]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "print('test')\n",
    "print(f'accuracy for topic {topic_dict_5[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_5[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4691 docs\n",
      "count_false = 934 docs\n",
      "accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "count_true = 1205 docs\n",
      "count_false = 201 docs\n",
      "accuracy = 0.86\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print('test')\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4691 docs\n",
      "count_false = 934 docs\n",
      "accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (bleu) = 0.66\n",
      "accuracy (rouge) = 0.73\n",
      "accuracy (jaccard) = 0.80\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "accuracy (bleu) = 0.68\n",
      "accuracy (rouge) = 0.73\n",
      "accuracy (jaccard) = 0.80\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "print('test')\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4159,  229],\n",
       "        [ 283,  954]],\n",
       "\n",
       "       [[2724,  205],\n",
       "        [1541, 1155]],\n",
       "\n",
       "       [[3488,  162],\n",
       "        [ 224, 1751]],\n",
       "\n",
       "       [[3283, 1360],\n",
       "        [ 177,  805]],\n",
       "\n",
       "       [[3746, 1288],\n",
       "        [ 146,  445]]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1080,   47],\n",
       "        [  59,  220]],\n",
       "\n",
       "       [[ 636,   45],\n",
       "        [ 423,  302]],\n",
       "\n",
       "       [[ 850,   41],\n",
       "        [  60,  455]],\n",
       "\n",
       "       [[ 806,  350],\n",
       "        [  38,  212]],\n",
       "\n",
       "       [[ 901,  334],\n",
       "        [  15,  156]]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('test')\n",
    "metrics.multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               labor       0.81      0.77      0.79      1237\n",
      "personal_right/court       0.85      0.43      0.57      2696\n",
      "   family/succession       0.92      0.89      0.90      1975\n",
      "            contract       0.37      0.82      0.51       982\n",
      "            criminal       0.26      0.75      0.38       591\n",
      "\n",
      "           micro avg       0.61      0.68      0.65      7481\n",
      "           macro avg       0.64      0.73      0.63      7481\n",
      "        weighted avg       0.75      0.68      0.67      7481\n",
      "         samples avg       0.66      0.73      0.66      7481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_names = list(topic_dict_5.values())\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               labor       0.82      0.79      0.81       279\n",
      "personal_right/court       0.87      0.42      0.56       725\n",
      "   family/succession       0.92      0.88      0.90       515\n",
      "            contract       0.38      0.85      0.52       250\n",
      "            criminal       0.32      0.91      0.47       171\n",
      "\n",
      "           micro avg       0.62      0.69      0.66      1940\n",
      "           macro avg       0.66      0.77      0.65      1940\n",
      "        weighted avg       0.76      0.69      0.67      1940\n",
      "         samples avg       0.68      0.73      0.67      1940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_names = list(topic_dict_5.values())\n",
    "print('test')\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic labor : 0.70\n",
      "accuracy for topic personal_right/court : 0.72\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.79\n",
      "accuracy for topic criminal : 0.74\n",
      "accuracy for topic labor : 0.72\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_6.pkl\", \"rb\") as f:\n",
    "    lda_model_6 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_6.pkl\", \"rb\") as f:\n",
    "    topic_dict_6 = pickle.load(f)\n",
    "num_topics = len(topic_dict_6)\n",
    "pred_question_score = [lda_model_6[text] for text in corpus_question_train]\n",
    "pred_answer_score = [lda_model_6[text] for text in corpus_answer_train]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "topic_true_5 = [sublist[5] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "topic_pred_5 = [sublist[5] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_6[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[5]} : {metrics.accuracy_score(topic_true_5, topic_pred_5):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4357 docs\n",
      "count_false = 1268 docs\n",
      "accuracy = 0.77\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (bleu) = 0.58\n",
      "accuracy (rouge) = 0.61\n",
      "accuracy (jaccard) = 0.77\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3387, 1395],\n",
       "        [ 178,  665]],\n",
       "\n",
       "       [[3173,  244],\n",
       "        [1452,  756]],\n",
       "\n",
       "       [[3450,  188],\n",
       "        [ 221, 1766]],\n",
       "\n",
       "       [[3369,  970],\n",
       "        [ 237, 1049]],\n",
       "\n",
       "       [[3704, 1335],\n",
       "        [ 127,  459]],\n",
       "\n",
       "       [[3854,   53],\n",
       "        [1494,  224]]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               labor       0.32      0.79      0.46       843\n",
      "personal_right/court       0.76      0.34      0.47      2208\n",
      "   family/succession       0.90      0.89      0.90      1987\n",
      "            contract       0.52      0.82      0.63      1286\n",
      "            criminal       0.26      0.78      0.39       586\n",
      "               labor       0.81      0.13      0.22      1718\n",
      "\n",
      "           micro avg       0.54      0.57      0.55      8628\n",
      "           macro avg       0.59      0.62      0.51      8628\n",
      "        weighted avg       0.69      0.57      0.54      8628\n",
      "         samples avg       0.58      0.61      0.56      8628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_names = list(topic_dict_6.values())\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i][1]==0): y_pred[i][0]=y_pred[i][5]\n",
    "if(len(y_pred[i]!=5)):\n",
    "    y_pred = np.delete(y_pred, 5, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(y_true)):\n",
    "    if(y_true[i][1]==0): y_true[i][1]=y_true[i][5]\n",
    "if(len(y_true[i]!=5)):\n",
    "    y_true = np.delete(y_true, 5, axis=1)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic labor : 0.50\n",
      "accuracy for topic personal_right/court : 0.81\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.79\n",
      "accuracy for topic criminal : 0.74\n"
     ]
    }
   ],
   "source": [
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_6[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               labor       0.27      0.17      0.21       843\n",
      "personal_right/court       0.87      0.25      0.38      3542\n",
      "   family/succession       0.90      0.89      0.90      1987\n",
      "            contract       0.52      0.82      0.63      1286\n",
      "            criminal       0.26      0.78      0.39       586\n",
      "\n",
      "           micro avg       0.59      0.52      0.55      8244\n",
      "           macro avg       0.56      0.58      0.50      8244\n",
      "        weighted avg       0.72      0.52      0.53      8244\n",
      "         samples avg       0.55      0.55      0.52      8244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_6.values())\n",
    "label_names = label_names[:5]\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic labor : 0.73\n",
      "accuracy for topic personal_right/court : 0.64\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.83\n",
      "accuracy for topic criminal : 0.68\n",
      "accuracy for topic contract : 0.74\n",
      "accuracy for topic labor : 0.87\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_7.pkl\", \"rb\") as f:\n",
    "    lda_model_7 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_7.pkl\", \"rb\") as f:\n",
    "    topic_dict_7 = pickle.load(f)\n",
    "num_topics = len(topic_dict_7)\n",
    "pred_question_score = [lda_model_7[text] for text in corpus_question_train]\n",
    "pred_answer_score = [lda_model_7[text] for text in corpus_answer_train]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "topic_true_5 = [sublist[5] for sublist in y_true]\n",
    "topic_true_6 = [sublist[6] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "topic_pred_5 = [sublist[5] for sublist in y_pred]\n",
    "topic_pred_6 = [sublist[6] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_7[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_7[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_7[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_7[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_7[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_7[5]} : {metrics.accuracy_score(topic_true_5, topic_pred_5):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_7[6]} : {metrics.accuracy_score(topic_true_6, topic_pred_6):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4410 docs\n",
      "count_false = 1215 docs\n",
      "accuracy = 0.78\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (bleu) = 0.55\n",
      "accuracy (rouge) = 0.60\n",
      "accuracy (jaccard) = 0.77\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               labor       0.25      0.80      0.38       795\n",
      "personal_right/court       0.79      0.34      0.48      1995\n",
      "   family/succession       0.88      0.92      0.90      1942\n",
      "            contract       0.58      0.74      0.65      1213\n",
      "            criminal       0.21      0.74      0.33       581\n",
      "            contract       0.74      0.13      0.22      1626\n",
      "               labor       0.77      0.40      0.53      1036\n",
      "\n",
      "           micro avg       0.52      0.55      0.53      9188\n",
      "           macro avg       0.60      0.58      0.50      9188\n",
      "        weighted avg       0.69      0.55      0.53      9188\n",
      "         samples avg       0.55      0.60      0.54      9188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_7.values())\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i][0]==0): y_pred[i][1]=y_pred[i][6]\n",
    "    if(y_pred[i][3]==0): y_pred[i][1]=y_pred[i][5]\n",
    "if(len(y_pred[i]!=5)):\n",
    "    y_pred = np.delete(y_pred, 6, axis=1)\n",
    "    y_pred = np.delete(y_pred, 5, axis=1)\n",
    "for i in range(len(y_true)):\n",
    "    if(y_true[i][0]==0): y_true[i][1]=y_true[i][6]\n",
    "    if(y_true[i][3]==0): y_true[i][1]=y_true[i][5]\n",
    "if(len(y_true[i]!=5)):\n",
    "    y_true = np.delete(y_true, 6, axis=1)\n",
    "    y_true = np.delete(y_true, 5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic labor : 0.77\n",
      "accuracy for topic personal_right/court : 0.64\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.83\n",
      "accuracy for topic criminal : 0.68\n"
     ]
    }
   ],
   "source": [
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_6[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_6[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "               labor       0.25      0.80      0.38       795\n",
      "personal_right/court       0.53      0.13      0.22      1290\n",
      "   family/succession       0.88      0.92      0.90      1942\n",
      "            contract       0.58      0.74      0.65      1213\n",
      "            criminal       0.21      0.74      0.33       581\n",
      "\n",
      "           micro avg       0.46      0.67      0.55      5821\n",
      "           macro avg       0.49      0.67      0.50      5821\n",
      "        weighted avg       0.59      0.67      0.57      5821\n",
      "         samples avg       0.48      0.55      0.49      5821\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_7.values())\n",
    "label_names = label_names[:5]\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic contract : 0.72\n",
      "accuracy for topic personal_right/court : 0.67\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic contract : 0.76\n",
      "accuracy for topic criminal : 0.78\n",
      "accuracy for topic contract : 0.73\n",
      "accuracy for topic labor : 0.88\n",
      "accuracy for topic labor : 0.83\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_8.pkl\", \"rb\") as f:\n",
    "    lda_model_8 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_8.pkl\", \"rb\") as f:\n",
    "    topic_dict_8 = pickle.load(f)\n",
    "num_topics = len(topic_dict_8)\n",
    "pred_question_score = [lda_model_8[text] for text in corpus_question_train]\n",
    "pred_answer_score = [lda_model_8[text] for text in corpus_answer_train]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "topic_true_5 = [sublist[5] for sublist in y_true]\n",
    "topic_true_6 = [sublist[6] for sublist in y_true]\n",
    "topic_true_7 = [sublist[7] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "topic_pred_5 = [sublist[5] for sublist in y_pred]\n",
    "topic_pred_6 = [sublist[6] for sublist in y_pred]\n",
    "topic_pred_7 = [sublist[7] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_8[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_8[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_8[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_8[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_8[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_8[5]} : {metrics.accuracy_score(topic_true_5, topic_pred_5):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_8[6]} : {metrics.accuracy_score(topic_true_6, topic_pred_6):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_8[7]} : {metrics.accuracy_score(topic_true_7, topic_pred_7):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4548 docs\n",
      "count_false = 1077 docs\n",
      "accuracy = 0.81\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (bleu) = 0.56\n",
      "accuracy (rouge) = 0.59\n",
      "accuracy (jaccard) = 0.79\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            contract       0.31      0.82      0.45       919\n",
      "personal_right/court       0.76      0.32      0.45      2052\n",
      "   family/succession       0.89      0.91      0.90      1963\n",
      "            contract       0.42      0.76      0.54      1042\n",
      "            criminal       0.26      0.71      0.38       533\n",
      "            contract       0.75      0.12      0.20      1650\n",
      "               labor       0.78      0.49      0.60      1017\n",
      "               labor       0.28      0.45      0.35       579\n",
      "\n",
      "           micro avg       0.51      0.54      0.53      9755\n",
      "           macro avg       0.56      0.57      0.48      9755\n",
      "        weighted avg       0.65      0.54      0.51      9755\n",
      "         samples avg       0.56      0.59      0.54      9755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_8.values())\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic contract : 0.72\n",
      "accuracy for topic personal_right/court : 0.66\n",
      "accuracy for topic family/succession : 0.93\n",
      "accuracy for topic criminal : 0.78\n",
      "accuracy for topic labor : 0.82\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            contract       0.53      0.90      0.67      2153\n",
      "personal_right/court       0.76      0.32      0.45      2052\n",
      "   family/succession       0.89      0.91      0.90      1963\n",
      "            criminal       0.26      0.71      0.38       533\n",
      "               labor       0.68      0.61      0.65      1475\n",
      "\n",
      "           micro avg       0.61      0.69      0.65      8176\n",
      "           macro avg       0.62      0.69      0.61      8176\n",
      "        weighted avg       0.69      0.69      0.64      8176\n",
      "         samples avg       0.66      0.73      0.66      8176\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i][0]==0): y_pred[i][0]=y_pred[i][3]\n",
    "    if(y_pred[i][0]==0): y_pred[i][0]=y_pred[i][5]\n",
    "    if(y_pred[i][6]==0): y_pred[i][6]=y_pred[i][7]\n",
    "y_pred = np.delete(y_pred, 7, axis=1)\n",
    "y_pred = np.delete(y_pred, 5, axis=1)\n",
    "y_pred = np.delete(y_pred, 3, axis=1)\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if(y_true[i][1]==0): y_true[i][0]=y_true[i][3]\n",
    "    if(y_true[i][0]==0): y_true[i][0]=y_true[i][5]\n",
    "    if(y_true[i][6]==0): y_true[i][6]=y_true[i][7]\n",
    "y_true = np.delete(y_true, 7, axis=1)\n",
    "y_true = np.delete(y_true, 5, axis=1)\n",
    "y_true = np.delete(y_true, 3, axis=1)\n",
    "\n",
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_8.values())\n",
    "label_names.pop(7)\n",
    "label_names.pop(5)\n",
    "label_names.pop(3)\n",
    "\n",
    "\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {label_names[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {label_names[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {label_names[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {label_names[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {label_names[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic contract : 0.75\n",
      "accuracy for topic personal_right/court : 0.69\n",
      "accuracy for topic family/succession : 0.90\n",
      "accuracy for topic contract : 0.78\n",
      "accuracy for topic criminal : 0.78\n",
      "accuracy for topic contract : 0.73\n",
      "accuracy for topic family/succession : 0.82\n",
      "accuracy for topic personal_right/court : 0.84\n",
      "accuracy for topic labor : 0.93\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_9.pkl\", \"rb\") as f:\n",
    "    lda_model_9 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_9.pkl\", \"rb\") as f:\n",
    "    topic_dict_9 = pickle.load(f)\n",
    "num_topics = len(topic_dict_9)\n",
    "pred_question_score = [lda_model_9[text] for text in corpus_question_train]\n",
    "pred_answer_score = [lda_model_9[text] for text in corpus_answer_train]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "topic_true_5 = [sublist[5] for sublist in y_true]\n",
    "topic_true_6 = [sublist[6] for sublist in y_true]\n",
    "topic_true_7 = [sublist[7] for sublist in y_true]\n",
    "topic_true_8 = [sublist[8] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "topic_pred_5 = [sublist[5] for sublist in y_pred]\n",
    "topic_pred_6 = [sublist[6] for sublist in y_pred]\n",
    "topic_pred_7 = [sublist[7] for sublist in y_pred]\n",
    "topic_pred_8 = [sublist[8] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_9[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[5]} : {metrics.accuracy_score(topic_true_5, topic_pred_5):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[6]} : {metrics.accuracy_score(topic_true_6, topic_pred_6):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[7]} : {metrics.accuracy_score(topic_true_7, topic_pred_7):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_9[8]} : {metrics.accuracy_score(topic_true_8, topic_pred_8):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4649 docs\n",
      "count_false = 976 docs\n",
      "accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (bleu) = 0.57\n",
      "accuracy (rouge) = 0.58\n",
      "accuracy (jaccard) = 0.80\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_bleu = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_bleu:.2f}\")\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_rouge = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_rouge:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            contract       0.33      0.81      0.47       932\n",
      "personal_right/court       0.71      0.30      0.42      1744\n",
      "   family/succession       0.78      0.91      0.84      1670\n",
      "            contract       0.42      0.72      0.53       982\n",
      "            criminal       0.26      0.70      0.38       545\n",
      "            contract       0.79      0.16      0.26      1732\n",
      "   family/succession       0.55      0.22      0.31      1051\n",
      "personal_right/court       0.31      0.33      0.32       643\n",
      "               labor       0.78      0.84      0.81       964\n",
      "\n",
      "           micro avg       0.51      0.53      0.52     10263\n",
      "           macro avg       0.55      0.55      0.48     10263\n",
      "        weighted avg       0.61      0.53      0.49     10263\n",
      "         samples avg       0.57      0.58      0.54     10263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_9.values())\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic contract : 0.61\n",
      "accuracy for topic personal_right/court : 0.74\n",
      "accuracy for topic family/succession : 0.79\n",
      "accuracy for topic criminal : 0.78\n",
      "accuracy for topic labor : 0.93\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            contract       0.67      0.88      0.76      2632\n",
      "personal_right/court       0.67      0.34      0.45      2645\n",
      "   family/succession       0.73      0.72      0.73      2214\n",
      "            criminal       0.26      0.70      0.38       545\n",
      "               labor       0.78      0.84      0.81       964\n",
      "\n",
      "           micro avg       0.63      0.67      0.65      9000\n",
      "           macro avg       0.62      0.70      0.63      9000\n",
      "        weighted avg       0.67      0.67      0.64      9000\n",
      "         samples avg       0.70      0.72      0.67      9000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i][0]==0): y_pred[i][0]=y_pred[i][3]\n",
    "    if(y_pred[i][0]==0): y_pred[i][0]=y_pred[i][5]\n",
    "    if(y_pred[i][1]==0): y_pred[i][1]=y_pred[i][7]\n",
    "    if(y_pred[i][2]==0): y_pred[i][2]=y_pred[i][6]\n",
    "y_pred = np.delete(y_pred, 7, axis=1)\n",
    "y_pred = np.delete(y_pred, 6, axis=1)\n",
    "y_pred = np.delete(y_pred, 5, axis=1)\n",
    "y_pred = np.delete(y_pred, 3, axis=1)\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if(y_true[i][0]==0): y_true[i][0]=y_true[i][3]\n",
    "    if(y_true[i][0]==0): y_true[i][0]=y_true[i][5]\n",
    "    if(y_true[i][1]==0): y_true[i][1]=y_true[i][6]\n",
    "    if(y_true[i][2]==0): y_true[i][2]=y_true[i][7]\n",
    "y_true = np.delete(y_true, 7, axis=1)\n",
    "y_true = np.delete(y_true, 6, axis=1)\n",
    "y_true = np.delete(y_true, 5, axis=1)\n",
    "y_true = np.delete(y_true, 3, axis=1)\n",
    "\n",
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_9.values())\n",
    "label_names.pop(7)\n",
    "label_names.pop(6)\n",
    "label_names.pop(5)\n",
    "label_names.pop(3)\n",
    "\n",
    "\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {label_names[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {label_names[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {label_names[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {label_names[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {label_names[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic contract : 0.75\n",
      "accuracy for topic personal_right/court : 0.70\n",
      "accuracy for topic family/succession : 0.90\n",
      "accuracy for topic contract : 0.66\n",
      "accuracy for topic criminal : 0.80\n",
      "accuracy for topic contract : 0.90\n",
      "accuracy for topic family/succession : 0.85\n",
      "accuracy for topic personal_right/court : 0.84\n",
      "accuracy for topic labor : 0.93\n",
      "accuracy for topic contract : 0.76\n"
     ]
    }
   ],
   "source": [
    "with open(\"lda/lda_model_10.pkl\", \"rb\") as f:\n",
    "    lda_model_10 = pickle.load(f)\n",
    "with open(f\"lda/topic_dict_10.pkl\", \"rb\") as f:\n",
    "    topic_dict_10 = pickle.load(f)\n",
    "num_topics = len(topic_dict_10)\n",
    "pred_question_score = [lda_model_10[text] for text in corpus_question_train]\n",
    "pred_answer_score = [lda_model_10[text] for text in corpus_answer_train]\n",
    "threshold = 1/num_topics\n",
    "question_predict=[]\n",
    "for each_topic in pred_question_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold): temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  question_predict.append(temp_pred)\n",
    "pd.DataFrame(question_predict)\n",
    "answer_predict=[]\n",
    "for each_topic in pred_answer_score:\n",
    "  temp_pred = []\n",
    "  for topic in each_topic:\n",
    "    if(topic[1]>threshold) : temp_pred.append(1)\n",
    "    else: temp_pred.append(0)\n",
    "  answer_predict.append(temp_pred)\n",
    "pd.DataFrame(answer_predict)\n",
    "y_true = np.array(answer_predict)\n",
    "y_pred = np.array(question_predict)\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "topic_true_5 = [sublist[5] for sublist in y_true]\n",
    "topic_true_6 = [sublist[6] for sublist in y_true]\n",
    "topic_true_7 = [sublist[7] for sublist in y_true]\n",
    "topic_true_8 = [sublist[8] for sublist in y_true]\n",
    "topic_true_9 = [sublist[9] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "topic_pred_5 = [sublist[5] for sublist in y_pred]\n",
    "topic_pred_6 = [sublist[6] for sublist in y_pred]\n",
    "topic_pred_7 = [sublist[7] for sublist in y_pred]\n",
    "topic_pred_8 = [sublist[8] for sublist in y_pred]\n",
    "topic_pred_9 = [sublist[9] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {topic_dict_10[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[5]} : {metrics.accuracy_score(topic_true_5, topic_pred_5):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[6]} : {metrics.accuracy_score(topic_true_6, topic_pred_6):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[7]} : {metrics.accuracy_score(topic_true_7, topic_pred_7):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[8]} : {metrics.accuracy_score(topic_true_8, topic_pred_8):.2f}')\n",
    "print(f'accuracy for topic {topic_dict_10[9]} : {metrics.accuracy_score(topic_true_8, topic_pred_9):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_true = 4494 docs\n",
      "count_false = 1131 docs\n",
      "accuracy = 0.80\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "pred_list = []\n",
    "true_list = []\n",
    "checker = False\n",
    "for i in range(len(y_true)):\n",
    "    checker = False\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j] == 1):\n",
    "            if(y_true[i][j] == y_pred[i][j]):\n",
    "                checker = True\n",
    "    if(checker == True):\n",
    "        count_true += 1\n",
    "        pred_list.append(y_pred[i])\n",
    "        true_list.append(y_true[i])\n",
    "    if(checker == False):\n",
    "        count_false += 1\n",
    "print(f'count_true = {count_true} docs\\ncount_false = {count_false} docs')\n",
    "print(f\"accuracy = {count_true/(count_true+count_false):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy (bleu) = 0.53\n",
      "accuracy (rouge) = 0.55\n",
      "accuracy (jaccard) = 0.80\n"
     ]
    }
   ],
   "source": [
    "y_true = answer_predict\n",
    "y_pred = question_predict\n",
    "accuracy_list_bleu = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_pred[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_true[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_bleu.append(check/count)\n",
    "avg_acc_list_2 = sum(accuracy_list_bleu)/len(y_pred)\n",
    "print(f\"accuracy (bleu) = {avg_acc_list_2:.2f}\")\n",
    "accuracy_list_rouge = []\n",
    "for i in range(len(y_pred)):\n",
    "    count = 0\n",
    "    check = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==1):\n",
    "            count += 1\n",
    "            if(y_pred[i][j]==1):\n",
    "                check += 1\n",
    "    accuracy_list_rouge.append(check/count)\n",
    "avg_acc_list_2 = sum(accuracy_list_rouge)/len(y_pred)\n",
    "print(f\"accuracy (rouge) = {avg_acc_list_2:.2f}\")\n",
    "\n",
    "accuraccy_list_jacc = []\n",
    "for i in range(len(y_pred)):\n",
    "    jacc = 0\n",
    "    for j in range(num_topics):\n",
    "        if(y_true[i][j]==y_pred[i][j]): jacc+=1\n",
    "    accuraccy_list_jacc.append(jacc/num_topics)\n",
    "avg_acc_list_jacc = sum(accuraccy_list_jacc)/len(y_pred)\n",
    "print(f\"accuracy (jaccard) = {avg_acc_list_jacc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            contract       0.33      0.79      0.46       936\n",
      "personal_right/court       0.66      0.29      0.40      1652\n",
      "   family/succession       0.82      0.86      0.84      1714\n",
      "            contract       0.19      0.77      0.30       533\n",
      "            criminal       0.29      0.66      0.40       570\n",
      "            contract       0.46      0.23      0.30       565\n",
      "   family/succession       0.53      0.28      0.36       888\n",
      "personal_right/court       0.33      0.30      0.32       702\n",
      "               labor       0.78      0.82      0.80       988\n",
      "            contract       0.83      0.18      0.29      2059\n",
      "\n",
      "           micro avg       0.47      0.49      0.48     10607\n",
      "           macro avg       0.52      0.52      0.45     10607\n",
      "        weighted avg       0.62      0.49      0.47     10607\n",
      "         samples avg       0.53      0.55      0.50     10607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_10.values())\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for topic contract : 0.63\n",
      "accuracy for topic personal_right/court : 0.71\n",
      "accuracy for topic family/succession : 0.78\n",
      "accuracy for topic criminal : 0.80\n",
      "accuracy for topic labor : 0.93\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            contract       0.66      0.88      0.76      2912\n",
      "personal_right/court       0.63      0.33      0.43      2400\n",
      "   family/succession       0.76      0.69      0.72      2312\n",
      "            criminal       0.29      0.66      0.40       570\n",
      "               labor       0.78      0.82      0.80       988\n",
      "\n",
      "           micro avg       0.64      0.67      0.65      9182\n",
      "           macro avg       0.62      0.68      0.62      9182\n",
      "        weighted avg       0.67      0.67      0.65      9182\n",
      "         samples avg       0.70      0.72      0.67      9182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if(y_pred[i][0]==0): y_pred[i][0]=y_pred[i][3]\n",
    "    if(y_pred[i][0]==0): y_pred[i][0]=y_pred[i][5]\n",
    "    if(y_pred[i][0]==0): y_pred[i][0]=y_pred[i][9]\n",
    "    if(y_pred[i][1]==0): y_pred[i][1]=y_pred[i][7]\n",
    "    if(y_pred[i][2]==0): y_pred[i][2]=y_pred[i][6]\n",
    "y_pred = np.delete(y_pred, 9, axis=1)\n",
    "y_pred = np.delete(y_pred, 7, axis=1)\n",
    "y_pred = np.delete(y_pred, 6, axis=1)\n",
    "y_pred = np.delete(y_pred, 5, axis=1)\n",
    "y_pred = np.delete(y_pred, 3, axis=1)\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    if(y_true[i][0]==0): y_true[i][0]=y_true[i][3]\n",
    "    if(y_true[i][0]==0): y_true[i][0]=y_true[i][5]\n",
    "    if(y_true[i][0]==0): y_true[i][0]=y_true[i][9]\n",
    "    if(y_true[i][1]==0): y_true[i][1]=y_true[i][6]\n",
    "    if(y_true[i][2]==0): y_true[i][2]=y_true[i][7]\n",
    "y_true = np.delete(y_true, 9, axis=1)\n",
    "y_true = np.delete(y_true, 7, axis=1)\n",
    "y_true = np.delete(y_true, 6, axis=1)\n",
    "y_true = np.delete(y_true, 5, axis=1)\n",
    "y_true = np.delete(y_true, 3, axis=1)\n",
    "\n",
    "metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "label_names = list(topic_dict_10.values())\n",
    "label_names.pop(9)\n",
    "label_names.pop(7)\n",
    "label_names.pop(6)\n",
    "label_names.pop(5)\n",
    "label_names.pop(3)\n",
    "\n",
    "\n",
    "topic_true_0 = [sublist[1] for sublist in y_true]\n",
    "topic_true_1 = [sublist[0] for sublist in y_true]\n",
    "topic_true_2 = [sublist[2] for sublist in y_true]\n",
    "topic_true_3 = [sublist[3] for sublist in y_true]\n",
    "topic_true_4 = [sublist[4] for sublist in y_true]\n",
    "\n",
    "topic_pred_0 = [sublist[1] for sublist in y_pred]\n",
    "topic_pred_1 = [sublist[0] for sublist in y_pred]\n",
    "topic_pred_2 = [sublist[2] for sublist in y_pred]\n",
    "topic_pred_3 = [sublist[3] for sublist in y_pred]\n",
    "topic_pred_4 = [sublist[4] for sublist in y_pred]\n",
    "\n",
    "print(f'accuracy for topic {label_names[0]} : {metrics.accuracy_score(topic_true_0, topic_pred_0):.2f}')\n",
    "print(f'accuracy for topic {label_names[1]} : {metrics.accuracy_score(topic_true_1, topic_pred_1):.2f}')\n",
    "print(f'accuracy for topic {label_names[2]} : {metrics.accuracy_score(topic_true_2, topic_pred_2):.2f}')\n",
    "print(f'accuracy for topic {label_names[3]} : {metrics.accuracy_score(topic_true_3, topic_pred_3):.2f}')\n",
    "print(f'accuracy for topic {label_names[4]} : {metrics.accuracy_score(topic_true_4, topic_pred_4):.2f}')\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred,target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
