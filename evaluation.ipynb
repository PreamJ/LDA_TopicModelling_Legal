{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "import random\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import MmCorpus\n",
    "import csv\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from sklearn import metrics\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/DatasetLegal.csv')\n",
    "with open('preprocessing/train_corpus.pkl', 'rb') as f:\n",
    "  train_corpus = pickle.load(f)\n",
    "with open('preprocessing/test_corpus.pkl', 'rb') as f:\n",
    "  test_corpus = pickle.load(f)\n",
    "with open('preprocessing/id2word.pkl', 'rb') as f:\n",
    "  id2word = pickle.load(f)\n",
    "with open('preprocessing/train_data.pkl', 'rb') as f:\n",
    "  train_data = pickle.load(f)\n",
    "lda_models = {}\n",
    "for i in range(5,31):\n",
    "  with open(f'ldamodel/lda_model_{i}.pkl', 'rb') as f:\n",
    "    lda_models[i] = pickle.load(f)\n",
    "question_corpus = []\n",
    "for text in train_data['question'].values.tolist():\n",
    "  vec = id2word.doc2bow(text)\n",
    "  question_corpus.append(vec)\n",
    "answer_corpus = []\n",
    "for text in train_data['answer'].values.tolist():\n",
    "  vec = id2word.doc2bow(text)\n",
    "  answer_corpus.append(vec)\n",
    "accuracy_file = \"evaluation/accuracy_file.csv\"\n",
    "cnfMx_file = 'evaluation/cnfMx_file.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(num_topics):\n",
    "    lda_model = lda_models[num_topics]\n",
    "    threshold = 1/num_topics\n",
    "    pred_question_score = [lda_model[text] for text in question_corpus]\n",
    "    pred_answer_score = [lda_model[text] for text in answer_corpus]\n",
    "    question_predict=[]\n",
    "    for each_topic in pred_question_score:\n",
    "        temp_pred = []\n",
    "        for topic in each_topic:\n",
    "            if(topic[1]>threshold): temp_pred.append(1)\n",
    "            else: temp_pred.append(0)\n",
    "        question_predict.append(temp_pred)\n",
    "    pd.DataFrame(question_predict)\n",
    "    answer_predict=[]\n",
    "    for each_topic in pred_answer_score:\n",
    "        temp_pred = []\n",
    "        for topic in each_topic:\n",
    "            if(topic[1]>threshold) : temp_pred.append(1)\n",
    "            else: temp_pred.append(0)\n",
    "        answer_predict.append(temp_pred)\n",
    "    pd.DataFrame(answer_predict)\n",
    "    # return [question_predict, answer_predict] which are tags of documents\n",
    "    return question_predict, answer_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(num_topics, arr):\n",
    "    '''\n",
    "    arr[1] = tag from answer = true\n",
    "    arr[0] = tag from question = pred\n",
    "    '''\n",
    "    y_true = np.array(arr[1])\n",
    "    y_pred = np.array(arr[0])\n",
    "    topic_true = {}\n",
    "    topic_pred = {}\n",
    "    for i in range(num_topics):\n",
    "        topic_true[i] = [sublist[i] for sublist in y_true]\n",
    "        topic_pred[i] = [sublist[i] for sublist in y_pred]\n",
    "    results = []\n",
    "    acc = []\n",
    "    for i in range(num_topics):\n",
    "        accuracy = metrics.accuracy_score(topic_true[i], topic_pred[i])\n",
    "        results.append(f\"{accuracy:.2f}\")\n",
    "        acc.append(accuracy)\n",
    "    avg_acc = sum(acc)/num_topics\n",
    "    results.append(avg_acc)\n",
    "    with open(accuracy_file, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # csv_writer.writerow([f'{num_topics}Topic', 'Accuracy'])\n",
    "        csv_writer.writerow(results)\n",
    "    print(f\"Accuracy have been written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnfMx(num_topics, arr):\n",
    "    '''\n",
    "    arr[1] = tag from answer = true\n",
    "    arr[0] = tag from question = pred\n",
    "    '''\n",
    "    confustion_matrics = metrics.multilabel_confusion_matrix(arr[1], arr[0])\n",
    "    classification_report = metrics.classification_report(arr[1], arr[0])\n",
    "    with open(cnfMx_file, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        # csv_writer.writerows([f'{num_topics}topics'])\n",
    "        csvfile.write(classification_report)\n",
    "        print(f\"Classification report has been written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,31):\n",
    "    arr = tag(i)\n",
    "    accuracy(i, arr)\n",
    "    cnfMx(i, arr)\n",
    "    print(f\"report haas been written {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(list_data):\n",
    "    corpus = []\n",
    "    for text in list_data:\n",
    "        vec = id2word.doc2bow(text)\n",
    "        corpus.append(vec)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Similarity = 0.9428\n",
      "Document 3: Similarity = 0.3333\n",
      "Document 2: Similarity = 0.2357\n"
     ]
    }
   ],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "def find_sim(corpus, query):\n",
    "    '''\n",
    "    ex.\n",
    "    query = [(1,1),(2,1),(3,2)]\n",
    "    corpus = [[(1,1),(2,1),(3,1)], [(2,1),(4,1),(5,1)], [(1,2), (5,1), (6,1)]]\n",
    "    corpus = train_corpus\n",
    "    '''\n",
    "    similarity_index = MatrixSimilarity(corpus)\n",
    "    similarities = similarity_index[query]\n",
    "    sorted_similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    "    for doc_index, similarity in sorted_similarities:\n",
    "        print(f\"Document {doc_index + 1}: Similarity = {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    [เนื่องจากว่า, คนอื่น, ลงชื่อ, ทำงาน, ทำงาน, บ...\n",
       "answer      [ทำงาน, คนอื่น, ลงชื่อ, ทำงาน, งง, เวลา, อ่ะ, ...\n",
       "Name: 3216, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[3216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
